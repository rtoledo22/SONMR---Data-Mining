import java.io.File;
import java.io.IOException;
import java.net.URI;
import java.util.*;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.io.NullWritable;

public class SONMR {

    public static class TokenizerMapper
            extends Mapper<Object, Text, Text, IntWritable> {

        private final static IntWritable one = new IntWritable(1);
        private final Text word = new Text();
        private int dataset_size = 1;
        private int transactions_per_block;
        private int min_supp;
        private double corr_factor;
        private HashMap<HashSet<Integer>, Integer> dictionary = new HashMap<HashSet<Integer>, Integer>();
        private HashSet<HashSet<Integer>> candidates = new HashSet<HashSet<Integer>>();
        private HashSet<Integer> origFreqItems = new HashSet<Integer>();
        private HashSet<HashSet<Integer>>setAboveThresh = new HashSet<HashSet<Integer>>();
        private HashSet<HashSet<Integer>> setOfTransactions = new HashSet<HashSet<Integer>>();
        private HashMap<HashSet<Integer>, Integer> dictionaries = new HashMap<HashSet<Integer>, Integer>();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            double double_first_map_thresh = corr_factor*((double)min_supp/dataset_size)*transactions_per_block;
            int first_map_thresh = (int) double_first_map_thresh;
            String line = value.toString();
            String[] s = line.split("\\s");
                for(String item : s){
                    HashSet<Integer> itemset = new HashSet<Integer>();
                    if (item != ""){
                        itemset.add(Integer.parseInt(item));
                        dictionary.merge(itemset, 1, (oldValue, newValue) -> oldValue + newValue);
                    }
                }

            Iterator dicItr = dictionary.entrySet().iterator();
            while (dicItr.hasNext()){
                Map.Entry<HashSet<Integer>, Integer> pair = (Map.Entry)dicItr.next();
                if(pair.getValue() >= first_map_thresh){
                    setAboveThresh.add(pair.getKey());
                    Object[] pairToArray = pair.getKey().toArray();
                    Integer[] pairInts = Arrays.copyOf(pairToArray, pairToArray.length, Integer[].class);
                    origFreqItems.add(pairInts[0]);
                    candidates.add(pair.getKey());
                    String itemsInItemset = "";
                    for (Integer item : pair.getKey()){
                        if (itemsInItemset.equals("")){
                            itemsInItemset = item.toString();
                        }
                        else{
                            itemsInItemset = itemsInItemset.concat(" " + item.toString());
                        }
                    }
                        word.set(itemsInItemset);
                        NullWritable noValue = NullWritable.get();
                        IntWritable support = new IntWritable(pair.getValue());
                        context.write(word, support);

                }
            }
            while (!candidates.isEmpty()){
                HashSet<HashSet<Integer>> badCandidates = new HashSet<HashSet<Integer>>();
                HashSet<HashSet<Integer>> potentialCandidates = new HashSet<HashSet<Integer>>();
                for (HashSet<Integer> candidate : candidates){
                    for (Integer addOn : origFreqItems){
                        HashSet<Integer> potentialCandidate = new HashSet<Integer>();
                        potentialCandidate.addAll(candidate);
                        if(potentialCandidate.contains(addOn) || badCandidates.add(potentialCandidate)){
                            badCandidates.add(potentialCandidate);
                        }
                        else{
                            potentialCandidate.add(addOn);
                            potentialCandidates.add(potentialCandidate);
                            badCandidates.add(potentialCandidate);
                        }
                    }
                }
                candidates.clear();
                dictionary.clear();
                for (HashSet<Integer> potentialCandidate : potentialCandidates){
                    if (!setAboveThresh.contains(potentialCandidate) || badCandidates.contains(potentialCandidate)){
                        candidates.add(potentialCandidate);
                    }
                }
                String[] t= line.split("\\R");
                String[] u;
                ArrayList<ArrayList<Integer>> checkComplexFrequent = new ArrayList<ArrayList<Integer>>();
                for (String transaction : t){
                    ArrayList<Integer> complexFrequent = new ArrayList<Integer>();
                    u = transaction.split("\\s");
                    for (String item : u){
                        complexFrequent.add(Integer.parseInt(item));
                    }
                    checkComplexFrequent.add(complexFrequent);
                }
                for (ArrayList<Integer> transaction : checkComplexFrequent){
                    for (HashSet<Integer> candidate : candidates){
                        if (transaction.containsAll(candidate)){
                            dictionary.merge(candidate, 1, (oldValue, newValue) -> oldValue + newValue);
                        }
                    }
                }
                dicItr = dictionary.entrySet().iterator();
                HashMap<Integer, Integer> frequent = new HashMap<Integer, Integer>();
                while(dicItr.hasNext()){
                    Map.Entry<HashSet<Integer>, Integer> pair = (Map.Entry)dicItr.next(); //DO SOMETHING ABOUT REMOVE
                    if (pair.getValue() >= first_map_thresh){
                        setAboveThresh.add(pair.getKey());
                        candidates.add(pair.getKey());
                        NullWritable noValue = NullWritable.get();
                        String itemsInItemset = "";
                        for (Integer frequentInt: pair.getKey()){
                            frequent.merge(frequentInt, 1, (oldValue, newValue) -> oldValue + newValue);
                        }
                        for (Integer item : pair.getKey()){
                            if (itemsInItemset.equals("")){
                                itemsInItemset = item.toString();
                            }
                            else{
                                itemsInItemset = itemsInItemset.concat(" " + item.toString());
                            }
                        }
                        word.set(itemsInItemset);
                        noValue = NullWritable.get();
                        IntWritable support = new IntWritable(pair.getValue());
                        context.write (word, support);
                    }
                }
                origFreqItems.clear();
                Iterator freqItr = frequent.entrySet().iterator();
                while (freqItr.hasNext()){
                    Map.Entry<Integer, Integer> pair = (Map.Entry)freqItr.next();
                    if(pair.getValue() < first_map_thresh){
                        origFreqItems.add(pair.getKey());
                    }
                }
            }
        }

        public void setup(Context context) throws IOException, InterruptedException{
            URI[] cacheFiles = context.getCacheFiles();
            Configuration conf = context.getConfiguration();
            dataset_size = conf.getInt("dataset_size", 1);
            transactions_per_block = conf.getInt("transactions_per_block", 1);
            min_supp = conf.getInt("min_supp", 1);
            corr_factor = conf.getDouble("corr_factor", 1);

        }
    }

    public static class IntSumReducer
            extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values,
                           Context context
        ) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            NullWritable noValue = NullWritable.get();
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        int dataset_size = Integer.parseInt(args[0]);
        int transactions_per_block = Integer.parseInt(args[1]);
        int min_supp = Integer.parseInt(args[2]);
        double corr_factor = Double.parseDouble(args[3]);
        Path input_path = new Path(args[4]);
        Configuration conf = new Configuration();

        conf.setInt("dataset_size", dataset_size);
        conf.setInt("transactions_per_block", transactions_per_block);
        conf.setInt("min_supp", min_supp);
        conf.setDouble("corr_factor", corr_factor);

        Job job = Job.getInstance(conf, "SONMR");
        job.setJarByClass(SONMR.class);
        job.setInputFormatClass(MultiLineInputFormat.class);
        org.apache.hadoop.mapreduce.lib.input.NLineInputFormat.setNumLinesPerSplit(job, transactions_per_block);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[4]));
        FileOutputFormat.setOutputPath(job, new Path(args[5]));
        job.addCacheFile(input_path.toUri());
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
